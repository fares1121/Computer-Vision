{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **CIFAR-10 Dataset Images Classification**"
      ],
      "metadata": {
        "id": "MRxGRk1R0WxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Model Using Keras"
      ],
      "metadata": {
        "id": "qHGNl4pd0k8E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will train and test a CNN model to classify images from CIFAR-10 dataset using Keras."
      ],
      "metadata": {
        "id": "BfFesXZ2064l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CIFAR-10 dataset consists of 60000 32 x 32 pixel color images in 10 classes, with 6000 images per class. Unlike the MNIST dataset, the CIFAR-10 images are colored and taken in varying lighting conditions and at different angles, resulting many variations in the color itself of similar objects. So, the simple CNN architecture used with the MNIST will have low validation accurancy.\n"
      ],
      "metadata": {
        "id": "fqHE8RQMJp8k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get a better accurancy we performed few changes in the model:\n",
        "- Increase the number of Conv2D layers to build a deeper model\n",
        "- Add Max pooling layer\n",
        "- Increase the number of filters to learn more features\n",
        "- Add Dropout for regularization and to reduce overfitting\n",
        "- Add more Dense layers"
      ],
      "metadata": {
        "id": "Pp3UZEyeM2Dk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries**"
      ],
      "metadata": {
        "id": "B_P8ZDZM1GD5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "EOF871nyuGiC"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten\n",
        "import keras.utils as np_utils"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Dataset**"
      ],
      "metadata": {
        "id": "wkCPqzYw2IWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jl5zoPiK13Vn",
        "outputId": "c974b256-3c17-49b9-972d-b6904405c64a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 11s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build Input Vector**"
      ],
      "metadata": {
        "id": "bocTNqZY2utl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will build an input vector from the 32x32 pixels."
      ],
      "metadata": {
        "id": "VkPJ4FXC2xn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 32, 32, 3)\n",
        "X_test = X_test.reshape(X_test.shape[0], 32, 32, 3)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')"
      ],
      "metadata": {
        "id": "CabJN6-R2_ZJ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perform Data Normalization**"
      ],
      "metadata": {
        "id": "pSFqiJ-P3PBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "metadata": {
        "id": "T14fS7Rr3ftj"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convert Label Vector Into One-hot Encodings**"
      ],
      "metadata": {
        "id": "BCfrcHfo4Mmr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use Keras np_utils to convert the labels into one-hot encoding vectors."
      ],
      "metadata": {
        "id": "FS3VS95b4Z61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_classes = 10\n",
        "print(\"Shape before one-hot encoding: \", y_train.shape)\n",
        "Y_train = np_utils.to_categorical(y_train, n_classes)\n",
        "Y_test = np_utils.to_categorical(y_test, n_classes)\n",
        "print(\"Shape after one-hot encoding: \", Y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2uxUA1-4td3",
        "outputId": "a0eff18b-8223-44c1-a922-b4450546ec86"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape before one-hot encoding:  (50000, 1)\n",
            "Shape after one-hot encoding:  (50000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build The CNN Model**"
      ],
      "metadata": {
        "id": "UtsmpNyl5Jl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(50, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(Conv2D(75, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv2D(125, kernel_size=(3,3), strides=(1,1), padding='same', activation='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(500, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(250, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(10, activation='softmax'))"
      ],
      "metadata": {
        "id": "c1cWNR6B5M5L"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compile The Model**"
      ],
      "metadata": {
        "id": "75Da8Zir5hTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use cross entropy loss and Adam optimizer to train the CNN model. The evaluation matrix will be accurancy."
      ],
      "metadata": {
        "id": "QD64wf0p6npg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
      ],
      "metadata": {
        "id": "BVTEpLwT5n7E"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train The Model**"
      ],
      "metadata": {
        "id": "yaROplW353oB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, Y_train, batch_size=128, epochs=10, validation_data=(X_test, Y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLOa6Afa6i7Q",
        "outputId": "6a3eeefc-0a6c-45e4-8a40-6c17ccd54389"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "391/391 [==============================] - 575s 1s/step - loss: 1.6169 - accuracy: 0.4034 - val_loss: 1.2332 - val_accuracy: 0.5682\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 548s 1s/step - loss: 1.1338 - accuracy: 0.5986 - val_loss: 0.9351 - val_accuracy: 0.6777\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 509s 1s/step - loss: 0.9351 - accuracy: 0.6748 - val_loss: 0.8398 - val_accuracy: 0.7089\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 510s 1s/step - loss: 0.8267 - accuracy: 0.7104 - val_loss: 0.7875 - val_accuracy: 0.7337\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 507s 1s/step - loss: 0.7360 - accuracy: 0.7434 - val_loss: 0.7199 - val_accuracy: 0.7510\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 505s 1s/step - loss: 0.6619 - accuracy: 0.7694 - val_loss: 0.6853 - val_accuracy: 0.7624\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 504s 1s/step - loss: 0.6114 - accuracy: 0.7851 - val_loss: 0.6903 - val_accuracy: 0.7644\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 546s 1s/step - loss: 0.5554 - accuracy: 0.8035 - val_loss: 0.6898 - val_accuracy: 0.7620\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 580s 1s/step - loss: 0.5151 - accuracy: 0.8183 - val_loss: 0.6708 - val_accuracy: 0.7727\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 548s 1s/step - loss: 0.4755 - accuracy: 0.8312 - val_loss: 0.6519 - val_accuracy: 0.7784\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ff0b54059f0>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accurancy after 10 epochs is 83.12% which is pretty decent."
      ],
      "metadata": {
        "id": "gxxKPXqPP05z"
      }
    }
  ]
}